{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4486555,"sourceType":"datasetVersion","datasetId":2625108},{"sourceId":11487278,"sourceType":"datasetVersion","datasetId":7200203},{"sourceId":11504648,"sourceType":"datasetVersion","datasetId":7213175},{"sourceId":345805,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":288930,"modelId":309676}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face Detection & Recognition","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"# Standard libraries\nimport os\nimport random\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# PyTorch core\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# PyTorch utilities\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# TorchVision\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.io import read_image\nfrom torchvision import datasets\n\n\n# Image processing\nfrom PIL import Image\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:59:11.796748Z","iopub.execute_input":"2025-04-21T17:59:11.797047Z","iopub.status.idle":"2025-04-21T17:59:11.802065Z","shell.execute_reply.started":"2025-04-21T17:59:11.797026Z","shell.execute_reply":"2025-04-21T17:59:11.801197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preparing The Dataset","metadata":{}},{"cell_type":"code","source":"class FaceDataset(Dataset):\n    def __init__(self, labels_file, images_dir, new_size=(256, 256), max_boxes=6, augment=False):\n        self.images_dir = images_dir\n        self.new_size = new_size\n        self.max_boxes = max_boxes\n        self.augment = augment \n\n        self.img_paths = []\n        self.img_coords = []\n\n        transform_list = [transforms.Resize(self.new_size)]\n\n        if self.augment:\n            transform_list.extend([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(10),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n            ])\n\n        transform_list.append(transforms.ToTensor())\n\n        self.transform = transforms.Compose(transform_list)\n\n        # Parsing the labels file\n        with open(labels_file, 'r') as f:\n            current = []\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                if line.startswith('#'):\n                    if current:\n                        self.img_coords.append(current)\n                        current = []\n                    self.img_paths.append(line.lstrip('# ').strip())\n                else:\n                    current.append(list(map(int, line.split())))\n            if current:\n                self.img_coords.append(current)\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.images_dir, self.img_paths[idx])\n        img = Image.open(path).convert('RGB')\n        ow, oh = img.size\n        img_t = self.transform(img)\n\n        bboxes = []\n        for x1, y1, x2, y2 in self.img_coords[idx]:\n            nx1 = x1 * self.new_size[0] / ow\n            ny1 = y1 * self.new_size[1] / oh\n            nx2 = x2 * self.new_size[0] / ow\n            ny2 = y2 * self.new_size[1] / oh\n            bboxes.append([nx1 / self.new_size[0],\n                           ny1 / self.new_size[1],\n                           nx2 / self.new_size[0],\n                           ny2 / self.new_size[1]])\n\n        padded = bboxes[:self.max_boxes] + [[-1] * 4] * (self.max_boxes - len(bboxes))\n        confs = [1.0] * min(len(bboxes), self.max_boxes) + [0.0] * (self.max_boxes - len(bboxes))\n\n        boxes_t = torch.tensor(padded, dtype=torch.float32)\n        confs_t = torch.tensor(confs, dtype=torch.float32).unsqueeze(-1)\n        target = torch.cat([boxes_t, confs_t], dim=-1)\n\n        return img_t, target\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T16:23:01.503682Z","iopub.execute_input":"2025-04-21T16:23:01.503952Z","iopub.status.idle":"2025-04-21T16:23:01.513989Z","shell.execute_reply.started":"2025-04-21T16:23:01.503931Z","shell.execute_reply":"2025-04-21T16:23:01.513134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = FaceDataset(\n    labels_file='/kaggle/input/dataset-for-face-detection/Dataset_FDDB/Dataset_FDDB/label.txt',\n    images_dir='/kaggle/input/dataset-for-face-detection/Dataset_FDDB/Dataset_FDDB/images'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:59:21.250282Z","iopub.execute_input":"2025-04-21T17:59:21.251125Z","iopub.status.idle":"2025-04-21T17:59:21.539081Z","shell.execute_reply.started":"2025-04-21T17:59:21.251090Z","shell.execute_reply":"2025-04-21T17:59:21.538285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plotting The Dataset","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(5, 5, figsize=(15, 15))\n\nfor i in range(25):\n    row = i // 5\n    col = i % 5\n    image_tensor, target = dataset[i]  \n    \n    bbox = target[:, :4]  # Shape (6, 4) [x1, y1, x2, y2] normalized\n    conf = target[:, 4]   # Shape (6,) confidence scores\n    \n    img = image_tensor.permute(1, 2, 0).numpy()  \n    \n    non_dummy_boxes = []\n    for box, c in zip(bbox, conf):\n        if c > 0.5:  # confidence > 50%\n            # denormalize\n            x1 = box[0].item() * 256\n            y1 = box[1].item() * 256\n            x2 = box[2].item() * 256\n            y2 = box[3].item() * 256\n            non_dummy_boxes.append([x1, y1, x2, y2])\n    \n    axs[row, col].imshow(img)\n    for box in non_dummy_boxes:\n        x1, y1, x2, y2 = box\n        rect = patches.Rectangle(\n            (x1, y1), x2-x1, y2-y1,\n            linewidth=2, edgecolor='r', facecolor='none'\n        )\n        axs[row, col].add_patch(rect)\n    axs[row, col].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:59:23.459432Z","iopub.execute_input":"2025-04-21T17:59:23.459694Z","iopub.status.idle":"2025-04-21T17:59:25.495041Z","shell.execute_reply.started":"2025-04-21T17:59:23.459674Z","shell.execute_reply":"2025-04-21T17:59:25.494065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_size = len(dataset)\ntrain_size = int(0.8 * total_size)  \nval_size = int(0.1 * total_size) \ntest_size = total_size - train_size - val_size \n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:59:34.010462Z","iopub.execute_input":"2025-04-21T17:59:34.010742Z","iopub.status.idle":"2025-04-21T17:59:34.016505Z","shell.execute_reply.started":"2025-04-21T17:59:34.010722Z","shell.execute_reply":"2025-04-21T17:59:34.015770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom CNN Architecture","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1   = nn.BatchNorm2d(out_channels)\n        self.relu  = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2   = nn.BatchNorm2d(out_channels)\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels or stride != 1:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(out_channels),\n            )\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return self.relu(out)\n\nclass FaceDetectorCNN(nn.Module):\n    def __init__(self, grid_size=8, max_boxes=6):\n        super().__init__()\n        self.max_boxes = max_boxes\n        # ---- feature extractor ----\n        self.backbone = nn.Sequential(\n            ResidualBlock(3, 16), nn.MaxPool2d(2),\n            ResidualBlock(16,32), nn.MaxPool2d(2),\n            ResidualBlock(32,64), ResidualBlock(64,64), nn.MaxPool2d(2),\n            ResidualBlock(64,128), ResidualBlock(128,128),\n            nn.AdaptiveAvgPool2d((grid_size, grid_size)),\n        )\n        feat_dim = 128 * grid_size * grid_size\n        # ---- heads ----\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(feat_dim, 512), nn.ReLU(),\n            nn.Linear(512, 256), nn.ReLU(),\n        )\n        # 5 outputs per box: 4 coordinates + 1 confidence\n        self.box_head  = nn.Linear(256, max_boxes * 5)  # [x1, y1, x2, y2, confidence]\n        \n    def forward(self, x):\n        f = self.backbone(x)\n        h = self.fc(f)\n        \n        # Predicted boxes + confidence\n        boxes_conf = self.box_head(h).view(-1, self.max_boxes, 5) \n\n        boxes = boxes_conf[..., :4]  # First 4 --> coords\n        conf_logits = boxes_conf[..., 4]  # 5th --> confidence \n        \n        return boxes, conf_logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:59:36.615272Z","iopub.execute_input":"2025-04-21T17:59:36.615551Z","iopub.status.idle":"2025-04-21T17:59:36.624787Z","shell.execute_reply.started":"2025-04-21T17:59:36.615529Z","shell.execute_reply":"2025-04-21T17:59:36.624016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detection_loss(pred_boxes, pred_conf_logits, targets,\n                   lambda_box=5.0, lambda_conf=1.0):\n    device = pred_conf_logits.device\n    B, M, _ = pred_boxes.shape  # B=batch size, M=max boxes\n\n    true_boxes = targets[..., :4]         # (B, M, 4)\n    true_conf  = targets[..., 4].float()  # (B, M)\n\n    # Box Loss: Smooth L1 only on positive samples \n    pos_mask = true_conf == 1             # (B, M)\n    num_pos = pos_mask.sum().float().clamp(min=1.0)\n\n    if num_pos > 0:\n        box_loss = F.smooth_l1_loss(\n            pred_boxes[pos_mask], true_boxes[pos_mask], reduction='sum'\n        ) / num_pos\n    else:\n        box_loss = torch.tensor(0., device=device)\n\n    # Confidence Loss: Balanced Binary Cross Entropy \n    pos_mask = true_conf == 1\n    neg_mask = true_conf == 0\n\n    pred_conf_pos = pred_conf_logits[pos_mask]\n    pred_conf_neg = pred_conf_logits[neg_mask]\n\n    true_conf_pos = true_conf[pos_mask]\n    true_conf_neg = true_conf[neg_mask]\n\n    # Compute losses\n    pos_loss = F.binary_cross_entropy_with_logits(\n        pred_conf_pos, true_conf_pos, reduction='sum'\n    )\n    neg_loss = F.binary_cross_entropy_with_logits(\n        pred_conf_neg, true_conf_neg, reduction='sum'\n    )\n\n    neg_weight = 0.25  \n    conf_loss = (pos_loss + neg_weight * neg_loss) / (pos_mask.sum() + neg_weight * neg_mask.sum()).clamp(min=1.0)\n\n    total_loss = lambda_box * box_loss + lambda_conf * conf_loss\n\n    return total_loss, {'box_loss': box_loss, 'conf_loss': conf_loss}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:59:49.815572Z","iopub.execute_input":"2025-04-21T17:59:49.816231Z","iopub.status.idle":"2025-04-21T17:59:49.822285Z","shell.execute_reply.started":"2025-04-21T17:59:49.816207Z","shell.execute_reply":"2025-04-21T17:59:49.821501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FaceDetectorCNN().eval()\n\nimgs, targets = next(iter(train_loader))\nwith torch.no_grad():\n    pred_boxes, pred_conf_logits = model(imgs)\n\npred_conf_probs = torch.softmax(pred_conf_logits, dim=-1)[..., 1]\n\n# Now you can index:\nprint(\"Pred boxes:\",        pred_boxes[0])       \nprint(\"Target boxes:\",      targets[0, :, :4])    \nprint(\"Pred conf (logits):\", pred_conf_logits[0])  \nprint(\"Pred conf (probs):\",  pred_conf_probs[0])  \nprint(\"Target conf:\",        targets[0, :, 4])    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T17:59:52.289535Z","iopub.execute_input":"2025-04-21T17:59:52.290238Z","iopub.status.idle":"2025-04-21T17:59:54.834780Z","shell.execute_reply.started":"2025-04-21T17:59:52.290210Z","shell.execute_reply":"2025-04-21T17:59:54.833990Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Computing Accuracy","metadata":{}},{"cell_type":"code","source":"def compute_iou(box1, box2):\n    x1, y1, x2, y2 = box1\n    x1_p, y1_p, x2_p, y2_p = box2\n\n    # compute intersection\n    xi1 = max(x1, x1_p)\n    yi1 = max(y1, y1_p)\n    xi2 = min(x2, x2_p)\n    yi2 = min(y2, y2_p)\n    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n\n    # compute union\n    box_area = (x2 - x1) * (y2 - y1)\n    pred_area = (x2_p - x1_p) * (y2_p - y1_p)\n    union_area = box_area + pred_area - inter_area\n\n    return inter_area / union_area if union_area > 0 else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:00:00.331864Z","iopub.execute_input":"2025-04-21T18:00:00.332466Z","iopub.status.idle":"2025-04-21T18:00:00.337587Z","shell.execute_reply.started":"2025-04-21T18:00:00.332441Z","shell.execute_reply":"2025-04-21T18:00:00.336720Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from torch.nn.utils import clip_grad_norm_\nfrom torchvision.ops import generalized_box_iou_loss\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = FaceDetectorCNN().to(device)\nopt = optim.Adam(model.parameters(), lr=1e-3)\n\ntrain_loss_list, val_loss_list, val_iou_list = [], [], []\nepochs = 15\nfor ep in range(1, epochs+1):\n    model.train()\n    total_train_loss = 0.0\n    for batch_idx, (imgs, targs) in enumerate(train_loader):\n        imgs, targs = imgs.to(device), targs.to(device)\n        boxes, conf_logits = model(imgs)\n        loss, _ = detection_loss(boxes, conf_logits, targs)\n\n        opt.zero_grad()\n        loss.backward()\n        # Prevent exploding gradients\n        clip_grad_norm_(model.parameters(), max_norm=1.0)\n        opt.step()\n\n        total_train_loss += loss.item()\n\n    avg_train = total_train_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    total_val_loss, iou_sum, iou_count = 0.0, 0.0, 0\n    with torch.no_grad():\n        for imgs, targs in val_loader:\n            imgs, targs = imgs.to(device), targs.to(device)\n            boxes, conf_logits = model(imgs)\n            loss_v, _ = detection_loss(boxes, conf_logits, targs)\n            total_val_loss += loss_v.item()\n\n            # IoU accuracy\n            probs = torch.sigmoid(conf_logits)\n            for b, c, t in zip(boxes, probs, targs):\n                preds = b[c > 0.5]\n                trues = t[t[...,4]>0][:,:4]\n                for pb in preds:\n                    best = max([compute_iou(pb.cpu().numpy(), tb.cpu().numpy())\n                                for tb in trues], default=0)\n                    if best > 0.5:\n                        iou_sum += best; iou_count += 1\n\n    avg_val = total_val_loss / len(val_loader)\n    avg_iou = iou_sum / max(iou_count, 1)\n\n    train_loss_list.append(avg_train)\n    val_loss_list.append(avg_val)\n    val_iou_list.append(avg_iou)\n\n    print(f\"Epoch {ep:02d}  Train: {avg_train:.4f}  Val: {avg_val:.4f}  IoU: {avg_iou:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:00:07.241629Z","iopub.execute_input":"2025-04-21T18:00:07.242394Z","iopub.status.idle":"2025-04-21T18:04:26.729307Z","shell.execute_reply.started":"2025-04-21T18:00:07.242368Z","shell.execute_reply":"2025-04-21T18:04:26.728499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs_range = range(1, epochs + 1)\n\n# Plot training and validation loss\nplt.figure(figsize=(10, 5))\nplt.plot(epochs_range, train_loss_list, label='Train Loss')\nplt.plot(epochs_range, val_loss_list, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training vs Validation Loss')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:04:34.042333Z","iopub.execute_input":"2025-04-21T18:04:34.043096Z","iopub.status.idle":"2025-04-21T18:04:34.208072Z","shell.execute_reply.started":"2025-04-21T18:04:34.043067Z","shell.execute_reply":"2025-04-21T18:04:34.207314Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predicting On Test Dataset","metadata":{}},{"cell_type":"code","source":"model.eval()\nimgs, targets = next(iter(test_loader))\nimgs, targets = imgs.to(device), targets.to(device)\n\nwith torch.no_grad():\n    pred_boxes, pred_conf_logits = model(imgs)  \npred_conf = torch.sigmoid(pred_conf_logits)  # Convert logits to probabilities\n\ncorrect = 0\ntotal = 0\n\n# Visualization setup\nfig, axs = plt.subplots(4, 4, figsize=(16, 16))\n\nfor i in range(16):  \n    ax = axs[i//4, i%4]\n    img = imgs[i].cpu().permute(1,2,0).numpy()\n    ax.imshow(img)\n    ax.axis('off')\n\n    gt = targets[i].cpu()\n    img_boxes = pred_boxes[i].cpu()\n    img_conf = pred_conf[i].cpu()\n\n    # Accuracy\n    gt_boxes = [box[:4].flatten().mul(256).tolist() for box in gt if torch.all(box[:4] != 0)]\n    pr_boxes = []\n    for box, conf in zip(img_boxes, img_conf):\n        if conf > 0.5:  \n            box_coords = box[:4].flatten().mul(256).tolist()\n            pr_boxes.append((box_coords, conf.item()))\n    \n    total += len(gt_boxes)\n\n    # Predictions \n    for box, conf in zip(img_boxes, img_conf):\n        if conf > 0.5:\n            x1, y1, x2, y2 = box * 256\n            # Draw bounding box\n            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n                                edgecolor='red', facecolor='none',\n                                linestyle='--', linewidth=2)\n            ax.add_patch(rect)\n            # Draw confidence score\n            ax.text(x1, y1 - 5, f'{conf:.2f}', color='red',\n                    fontsize=10, fontweight='bold',\n                    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:04:40.568006Z","iopub.execute_input":"2025-04-21T18:04:40.568733Z","iopub.status.idle":"2025-04-21T18:04:42.426656Z","shell.execute_reply.started":"2025-04-21T18:04:40.568706Z","shell.execute_reply":"2025-04-21T18:04:42.425849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_confidences = pred_conf.cpu().flatten().numpy()\n\nplt.hist(all_confidences, bins=50)\nplt.title(\"Confidence score distribution\")\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:04:47.451495Z","iopub.execute_input":"2025-04-21T18:04:47.452100Z","iopub.status.idle":"2025-04-21T18:04:47.622894Z","shell.execute_reply.started":"2025-04-21T18:04:47.452074Z","shell.execute_reply":"2025-04-21T18:04:47.622109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using Pretrained Model ( YOLOv8 ) For detection","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics &> /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:05:06.323481Z","iopub.execute_input":"2025-04-21T18:05:06.324225Z","iopub.status.idle":"2025-04-21T18:05:09.480416Z","shell.execute_reply.started":"2025-04-21T18:05:06.324200Z","shell.execute_reply":"2025-04-21T18:05:09.479389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:05:23.750684Z","iopub.execute_input":"2025-04-21T18:05:23.751523Z","iopub.status.idle":"2025-04-21T18:05:23.755375Z","shell.execute_reply.started":"2025-04-21T18:05:23.751492Z","shell.execute_reply":"2025-04-21T18:05:23.754608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YOLO(\"/kaggle/input/yolo/pytorch/default/1/yolov8l_100e.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:05:36.507992Z","iopub.execute_input":"2025-04-21T18:05:36.508275Z","iopub.status.idle":"2025-04-21T18:05:36.688608Z","shell.execute_reply.started":"2025-04-21T18:05:36.508254Z","shell.execute_reply":"2025-04-21T18:05:36.688012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows, cols = 4, 4\n\nplt.figure(figsize=(cols * 4, rows * 4)) \n\nfor idx in range(16):\n    img_t, _ = test_dataset[idx]\n    img_np = (img_t.permute(1,2,0).numpy() * 255).astype(np.uint8)\n    img_np = np.ascontiguousarray(img_np)\n\n    results = model(img_np, conf=0.25)\n\n    annotated = results[0].plot()\n    plt.subplot(rows, cols, idx + 1)\n    plt.imshow(annotated)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:05:39.231080Z","iopub.execute_input":"2025-04-21T18:05:39.231733Z","iopub.status.idle":"2025-04-21T18:05:41.879859Z","shell.execute_reply.started":"2025-04-21T18:05:39.231707Z","shell.execute_reply":"2025-04-21T18:05:41.878673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\ninput_root = Path(\"/kaggle/input/facedataset/facesDS\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:05:51.562823Z","iopub.execute_input":"2025-04-21T18:05:51.563542Z","iopub.status.idle":"2025-04-21T18:05:51.566997Z","shell.execute_reply.started":"2025-04-21T18:05:51.563518Z","shell.execute_reply":"2025-04-21T18:05:51.566228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_root = Path(\"cropped_faces\")\noutput_root.mkdir(parents=True, exist_ok=True)\n\nskipped = 0\nprocessed = 0\n\nfor person_folder in input_root.iterdir():\n    if not person_folder.is_dir():\n        continue\n    person_name = person_folder.name\n    (output_root / person_name).mkdir(parents=True, exist_ok=True)\n\n    for img_file in person_folder.glob(\"*\"):\n        if img_file.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n            continue\n        img = cv2.imread(str(img_file))\n        results = model(img)[0]\n\n        # Get largest box only\n        boxes = results.boxes\n\n        if len(boxes) == 0:\n            skipped += 1\n            continue\n        processed += 1\n            \n\n        largest_box = max(boxes, key=lambda b: (b.xyxy[0][2] - b.xyxy[0][0]) * (b.xyxy[0][3] - b.xyxy[0][1]))\n        x1, y1, x2, y2 = map(int, largest_box.xyxy[0])\n        face_crop = img[y1:y2, x1:x2]\n\n        # Save cropped face\n        save_path = output_root / person_name / img_file.name\n        cv2.imwrite(str(save_path), face_crop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:05:55.197437Z","iopub.execute_input":"2025-04-21T18:05:55.198186Z","iopub.status.idle":"2025-04-21T18:06:19.488388Z","shell.execute_reply.started":"2025-04-21T18:05:55.198159Z","shell.execute_reply":"2025-04-21T18:06:19.487840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_count = sum(1 for f in input_root.rglob(\"*\") if f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"])\nprint(\"Total images:\", image_count)\nprint(f\"Processed: {processed} | Skipped: {skipped}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:06:46.284903Z","iopub.execute_input":"2025-04-21T18:06:46.285662Z","iopub.status.idle":"2025-04-21T18:06:46.595298Z","shell.execute_reply.started":"2025-04-21T18:06:46.285632Z","shell.execute_reply":"2025-04-21T18:06:46.594558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cropped_dir = \"/kaggle/working/cropped_faces\"\n\nrows, cols = 3, 5\nfig, axs = plt.subplots(rows, cols, figsize=(15, 9))\n\npeople = os.listdir(cropped_dir)  # list of person folders\n\ni = 0\nfor person in people:\n    person_folder = os.path.join(cropped_dir, person)\n    images = os.listdir(person_folder)[:cols]  # take first few images\n\n    for img_file in images:\n        if i >= rows * cols:\n            break\n        img_path = os.path.join(person_folder, img_file)\n        img = Image.open(img_path)\n\n        ax = axs[i // cols, i % cols]\n        ax.imshow(img)\n        ax.set_title(person)\n        ax.axis('off')\n        i += 1\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:06:48.698739Z","iopub.execute_input":"2025-04-21T18:06:48.699352Z","iopub.status.idle":"2025-04-21T18:06:50.185943Z","shell.execute_reply.started":"2025-04-21T18:06:48.699325Z","shell.execute_reply":"2025-04-21T18:06:50.185057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = \"/kaggle/working/cropped_faces\"\ndataset = datasets.ImageFolder(data_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:06:54.972190Z","iopub.execute_input":"2025-04-21T18:06:54.972472Z","iopub.status.idle":"2025-04-21T18:06:54.978323Z","shell.execute_reply.started":"2025-04-21T18:06:54.972451Z","shell.execute_reply":"2025-04-21T18:06:54.977591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"simple_transform = transforms.Compose([\n    transforms.Resize((224, 224)),   \n    transforms.ToTensor(),           \n])\n\ndataset.transform = simple_transform\n\ndef compute_mean_std(dataset):\n    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n    mean = 0.\n    std = 0.\n    total_images = 0\n\n    for images, _ in loader:\n        batch_samples = images.size(0)\n        images = images.view(batch_samples, images.size(1), -1)\n        mean += images.mean(2).sum(0)\n        std += images.std(2).sum(0)\n        total_images += batch_samples\n\n    mean /= total_images\n    std /= total_images\n    return mean, std\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:06:58.503260Z","iopub.execute_input":"2025-04-21T18:06:58.503549Z","iopub.status.idle":"2025-04-21T18:06:58.509656Z","shell.execute_reply.started":"2025-04-21T18:06:58.503527Z","shell.execute_reply":"2025-04-21T18:06:58.508733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean, std = compute_mean_std(dataset)\nprint(\"Mean:\", mean)\nprint(\"Std:\", std)\n\ndataset.transform = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:07:04.021071Z","iopub.execute_input":"2025-04-21T18:07:04.021675Z","iopub.status.idle":"2025-04-21T18:07:05.513521Z","shell.execute_reply.started":"2025-04-21T18:07:04.021646Z","shell.execute_reply":"2025-04-21T18:07:05.512542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_size = len(dataset)\ntrain_size = int(0.8 * total_size)  \nval_size = int(0.1 * total_size)    \ntest_size = total_size - train_size - val_size  \n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# Data Augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ndefault_transform = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)  \n\n])\n\ndef apply_transform(batch, transform):\n    transformed_images = [transform(img) for (img, label) in batch]\n    labels = [label for (img, label) in batch]\n    return torch.stack(transformed_images), torch.tensor(labels)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,\n    collate_fn=lambda x: apply_transform(x, train_transform),\n)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=32,\n    shuffle=False,\n    collate_fn=lambda x: apply_transform(x, default_transform),\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=32,\n    shuffle=False,\n    collate_fn=lambda x: apply_transform(x, default_transform),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:07:08.074607Z","iopub.execute_input":"2025-04-21T18:07:08.075348Z","iopub.status.idle":"2025-04-21T18:07:08.087833Z","shell.execute_reply.started":"2025-04-21T18:07:08.075314Z","shell.execute_reply":"2025-04-21T18:07:08.086978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print((dataset))\nprint(len(train_dataset))\nprint(len(val_dataset))\nprint(len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:07:16.097681Z","iopub.execute_input":"2025-04-21T18:07:16.097992Z","iopub.status.idle":"2025-04-21T18:07:16.102687Z","shell.execute_reply.started":"2025-04-21T18:07:16.097945Z","shell.execute_reply":"2025-04-21T18:07:16.101929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom CNN Model For Recognition","metadata":{}},{"cell_type":"code","source":"class classifierCNN(nn.Module):\n    def __init__(self, num_classes=3):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            # Block 2\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            # Block 3\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            # Block 4\n            nn.Conv2d(256, 512, 3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            \n            nn.Conv2d(512, 512, 3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.encoder(x).flatten(1)\n        return self.classifier(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:07:26.163615Z","iopub.execute_input":"2025-04-21T18:07:26.164225Z","iopub.status.idle":"2025-04-21T18:07:26.170500Z","shell.execute_reply.started":"2025-04-21T18:07:26.164200Z","shell.execute_reply":"2025-04-21T18:07:26.169650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:07:28.490809Z","iopub.execute_input":"2025-04-21T18:07:28.491115Z","iopub.status.idle":"2025-04-21T18:07:28.495055Z","shell.execute_reply.started":"2025-04-21T18:07:28.491094Z","shell.execute_reply":"2025-04-21T18:07:28.494219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = classifierCNN(num_classes=3).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\ntrain_loss_list = []\nval_loss_list   = []\ntrain_acc_list  = []\nval_acc_list    = []\nprecision_list  = []\nrecall_list     = []\nepochs = 40\n\nscaler = torch.cuda.amp.GradScaler()\n\nbest_val_loss = float('inf')\nearly_stop_counter = 0\npatience = 5\n\ntotal_start_time_1 = time.time() \n\nfor epoch in range(epochs):\n    # Training \n    model.train()\n    train_loss, train_correct, train_total = 0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            logits = model(images)\n            loss = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item() * labels.size(0)\n        train_correct += (logits.argmax(1) == labels).sum().item()\n        train_total += labels.size(0)\n\n    train_loss /= train_total\n    train_acc = train_correct / train_total\n\n    # Validation \n    model.eval()\n    all_labels, all_preds = [], []\n    val_loss, val_correct, val_total = 0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            logits = model(images)\n            loss = criterion(logits, labels)\n\n            val_loss += loss.item() * labels.size(0)\n            _, preds = logits.max(1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_loss /= val_total\n    val_acc = val_correct / val_total\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    # Scheduler step on validation loss\n    scheduler.step(val_loss)\n\n    # Record metrics\n    train_loss_list.append(train_loss)\n    train_acc_list.append(train_acc)\n    val_loss_list.append(val_loss)\n    val_acc_list.append(val_acc)\n    precision_list.append(precision)\n    recall_list.append(recall)\n\n    # Print epoch summary\n    print(f\"Epoch {epoch+1:02d}/{epochs} | \"\n          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n          f\"Val Prec: {precision:.4f} | Val Rec: {recall:.4f}\")\n\n    # Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stop_counter = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n\ntotal_time_1 = time.time() - total_start_time_1\nprint(f\"\\nTotal training time: {total_time_1:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:07:30.729827Z","iopub.execute_input":"2025-04-21T18:07:30.730744Z","iopub.status.idle":"2025-04-21T18:11:03.417732Z","shell.execute_reply.started":"2025-04-21T18:07:30.730721Z","shell.execute_reply":"2025-04-21T18:11:03.416814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = range(1, len(train_loss_list) + 1)\n\n# 1) Loss\nplt.figure()\nplt.plot(epochs, train_loss_list,    label='Train Loss')\nplt.plot(epochs, val_loss_list,      label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs. Epoch')\nplt.legend()\nplt.show()\n\n# 2) Accuracy\nplt.figure()\nplt.plot(epochs, train_acc_list,     label='Train Acc')\nplt.plot(epochs, val_acc_list,       label='Val Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. Epoch')\nplt.legend()\nplt.show()\n\n# 3) Precision & Recall\nplt.figure()\nplt.plot(epochs, precision_list,     label='Precision')\nplt.plot(epochs, recall_list,        label='Recall')\nplt.xlabel('Epoch')\nplt.ylabel('Score')\nplt.title('Precision/Recall vs. Epoch')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:11:08.266309Z","iopub.execute_input":"2025-04-21T18:11:08.267176Z","iopub.status.idle":"2025-04-21T18:11:08.740933Z","shell.execute_reply.started":"2025-04-21T18:11:08.267138Z","shell.execute_reply":"2025-04-21T18:11:08.740068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\n\n# Plot heatmap\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Print classification report\nprint(classification_report(all_labels, all_preds, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:11:14.475394Z","iopub.execute_input":"2025-04-21T18:11:14.475675Z","iopub.status.idle":"2025-04-21T18:11:14.586442Z","shell.execute_reply.started":"2025-04-21T18:11:14.475655Z","shell.execute_reply":"2025-04-21T18:11:14.585769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denormalize(tensor, mean, std):\n    denorm = tensor.clone()\n    for t, m, s in zip(denorm, mean, std):\n        t.mul_(s).add_(m)  # Reverse: (tensor * std) + mean\n    return denorm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:11:23.362627Z","iopub.execute_input":"2025-04-21T18:11:23.363328Z","iopub.status.idle":"2025-04-21T18:11:23.367333Z","shell.execute_reply.started":"2025-04-21T18:11:23.363300Z","shell.execute_reply":"2025-04-21T18:11:23.366535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = [\"Ahmed Zewail\", \"Farouk El-Baz\", \"Magdi Yacoub\"]  \n\n\nmodel.eval()\ntest_correct, test_total = 0, 0\n\ntest_images = []\ntest_labels = []\ntest_preds = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        logits  = model(images)\n        preds = logits.argmax(1)\n        \n        test_correct += (preds == labels).sum().item()\n        test_total += labels.size(0)\n        \n        test_images.append(images.cpu())\n        test_labels.append(labels.cpu())\n        test_preds.append(preds.cpu())\n\ntest_images = torch.cat(test_images, dim=0)\ntest_labels = torch.cat(test_labels, dim=0)\ntest_preds = torch.cat(test_preds, dim=0)\n\ndef imshow(img, title):\n    img = denormalize(img, \n                      mean=mean, \n                      std=std)\n    \n    np_img = img.numpy().transpose((1, 2, 0))  # (C, H, W) → (H, W, C)\n    \n    np_img = np.clip(np_img, 0, 1)\n    \n    plt.imshow(np_img)\n    plt.title(title)\n    plt.axis('off')\n\nplt.figure(figsize=(12, 12))  \n\nfor i in range(16):\n    plt.subplot(4, 4, i+1)  \n    img = test_images[i]\n    true_label = test_labels[i].item()\n    pred_label = test_preds[i].item()\n    \n    true_name = f\"True: {class_names[true_label]}\"\n    pred_name = f\"Pred: {class_names[pred_label]}\"\n    \n    imshow(img, f\"{true_name}\\n{pred_name}\")\n\nplt.show()\n\ntest_acc = test_correct / test_total\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:11:26.479120Z","iopub.execute_input":"2025-04-21T18:11:26.479742Z","iopub.status.idle":"2025-04-21T18:11:27.885829Z","shell.execute_reply.started":"2025-04-21T18:11:26.479718Z","shell.execute_reply":"2025-04-21T18:11:27.885133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using Pretrained Model ( ResNet50 ) For Recognition","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\n\nmodel_2 = models.resnet50(pretrained=True)\n\n# Freeze all layers\nfor param in model_2.parameters():\n    param.requires_grad = False\n\n# Replace final classification layer\nnum_classes = 3 \nmodel_2.fc = nn.Linear(model_2.fc.in_features, num_classes)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_2 = model_2.to(device)\n\noptimizer = torch.optim.Adam(model_2.fc.parameters(), lr=1e-3)\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:11:38.279891Z","iopub.execute_input":"2025-04-21T18:11:38.280469Z","iopub.status.idle":"2025-04-21T18:11:38.794108Z","shell.execute_reply.started":"2025-04-21T18:11:38.280443Z","shell.execute_reply":"2025-04-21T18:11:38.793348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loss_list_2 = []\nval_loss_list_2   = []\ntrain_acc_list_2  = []\nval_acc_list_2    = []\nprecision_list_2  = []\nrecall_list_2     = []\nepochs = 30\n\nscaler = torch.cuda.amp.GradScaler()\n\nbest_val_loss = float('inf')\nearly_stop_counter = 0\npatience = 5\n\ntotal_start_time_2 = time.time() \n\nfor epoch in range(epochs):\n    # Training\n    model_2.train()\n    train_loss, train_correct, train_total = 0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            logits = model_2(images)\n            loss = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model_2.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item() * labels.size(0)\n        train_correct += (logits.argmax(1) == labels).sum().item()\n        train_total += labels.size(0)\n\n    train_loss /= train_total\n    train_acc = train_correct / train_total\n\n    # Validation \n    model.eval()\n    all_labels, all_preds = [], []\n    val_loss, val_correct, val_total = 0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            logits = model_2(images)\n            loss = criterion(logits, labels)\n\n            val_loss += loss.item() * labels.size(0)\n            _, preds = logits.max(1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_loss /= val_total\n    val_acc = val_correct / val_total\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    # Scheduler step on validation loss\n    scheduler.step(val_loss)\n\n    # Record metrics\n    train_loss_list_2.append(train_loss)\n    train_acc_list_2.append(train_acc)\n    val_loss_list_2.append(val_loss)\n    val_acc_list_2.append(val_acc)\n    precision_list_2.append(precision)\n    recall_list_2.append(recall)\n\n    # Print epoch summary\n    print(f\"Epoch {epoch+1:02d}/{epochs} | \"\n          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n          f\"Val Prec: {precision:.4f} | Val Rec: {recall:.4f}\")\n\n    # Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stop_counter = 0\n        torch.save(model_2.state_dict(), 'best_model.pth')\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n\ntotal_time_2 = time.time() - total_start_time_2\nprint(f\"\\nTotal training time: {total_time_2:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:11:43.291686Z","iopub.execute_input":"2025-04-21T18:11:43.292393Z","iopub.status.idle":"2025-04-21T18:13:24.326976Z","shell.execute_reply.started":"2025-04-21T18:11:43.292366Z","shell.execute_reply":"2025-04-21T18:13:24.326357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = range(1, len(train_loss_list_2) + 1)\n\n# 1) Loss\nplt.figure()\nplt.plot(epochs, train_loss_list_2,    label='Train Loss')\nplt.plot(epochs, val_loss_list_2,      label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs. Epoch')\nplt.legend()\nplt.show()\n\n# 2) Accuracy\nplt.figure()\nplt.plot(epochs, train_acc_list_2,     label='Train Acc')\nplt.plot(epochs, val_acc_list_2,       label='Val Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. Epoch')\nplt.legend()\nplt.show()\n\n# 3) Precision & Recall\nplt.figure()\nplt.plot(epochs, precision_list_2,     label='Precision')\nplt.plot(epochs, recall_list_2,        label='Recall')\nplt.xlabel('Epoch')\nplt.ylabel('Score')\nplt.title('Precision/Recall vs. Epoch')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:13:28.496935Z","iopub.execute_input":"2025-04-21T18:13:28.497683Z","iopub.status.idle":"2025-04-21T18:13:28.955717Z","shell.execute_reply.started":"2025-04-21T18:13:28.497652Z","shell.execute_reply":"2025-04-21T18:13:28.954887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = [\"Ahmed Zewail\", \"Farouk El-Baz\", \"Magdi Yacoub\"]  \n\n\nmodel_2.eval()\ntest_correct, test_total = 0, 0\n\ntest_images = []\ntest_labels = []\ntest_preds = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        logits  = model_2(images)\n        preds = logits.argmax(1)\n        \n        test_correct += (preds == labels).sum().item()\n        test_total += labels.size(0)\n        \n        test_images.append(images.cpu())\n        test_labels.append(labels.cpu())\n        test_preds.append(preds.cpu())\n\ntest_images = torch.cat(test_images, dim=0)\ntest_labels = torch.cat(test_labels, dim=0)\ntest_preds = torch.cat(test_preds, dim=0)\n\ndef imshow(img, title):\n    img = denormalize(img, \n                      mean=mean, \n                      std=std)\n    \n    np_img = img.numpy().transpose((1, 2, 0))  # (C, H, W) → (H, W, C)\n    \n    np_img = np.clip(np_img, 0, 1)\n    \n    plt.imshow(np_img)\n    plt.title(title)\n    plt.axis('off')\n\nplt.figure(figsize=(12, 12)) \nfor i in range(16):\n    plt.subplot(4, 4, i+1)  \n    img = test_images[i]\n    true_label = test_labels[i].item()\n    pred_label = test_preds[i].item()\n    \n    true_name = f\"True: {class_names[true_label]}\"\n    pred_name = f\"Pred: {class_names[pred_label]}\"\n    \n    imshow(img, f\"{true_name}\\n{pred_name}\")\n\nplt.show()\n\ntest_acc = test_correct / test_total\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:13:36.858456Z","iopub.execute_input":"2025-04-21T18:13:36.859215Z","iopub.status.idle":"2025-04-21T18:13:38.313842Z","shell.execute_reply.started":"2025-04-21T18:13:36.859188Z","shell.execute_reply":"2025-04-21T18:13:38.313097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YOLO(\"/kaggle/input/yolo/pytorch/default/1/yolov8l_100e.pt\")\nrecognizer = model_2\nprint(detector)\nprint(recognizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:14:02.429639Z","iopub.execute_input":"2025-04-21T18:14:02.429935Z","iopub.status.idle":"2025-04-21T18:14:02.872666Z","shell.execute_reply.started":"2025-04-21T18:14:02.429912Z","shell.execute_reply":"2025-04-21T18:14:02.871858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\nvideo_path = \"/kaggle/input/testvid1/test1.mp4\"\ncap = cv2.VideoCapture(video_path)\nfps    = cap.get(cv2.CAP_PROP_FPS)\nw      = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nh      = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout    = cv2.VideoWriter(\"/kaggle/working/output1.avi\", fourcc, fps, (w, h))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    results = detector(frame)[0]\n    boxes   = results.boxes.xyxy.cpu().numpy()\n\n    for (x1, y1, x2, y2) in boxes.astype(int):\n        crop = frame[y1:y2, x1:x2]\n        if crop.size == 0:\n            continue\n\n        crop_rgb   = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n        crop_pil   = Image.fromarray(crop_rgb)\n        tensor     = transform(crop_pil).unsqueeze(0).cuda()\n\n        with torch.no_grad():\n            logits = recognizer(tensor)\n            idx    = logits.argmax(dim=1).item()\n            label  = class_names[idx]\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,0,255), 2)\n        cv2.putText(frame, label, (x1, y1-10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\n\nprint(\" Done — video saved to /kaggle/working/output.avi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:16:52.851225Z","iopub.execute_input":"2025-04-21T18:16:52.851537Z"}},"outputs":[],"execution_count":null}]}